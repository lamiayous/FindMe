{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f44ea31b",
   "metadata": {},
   "source": [
    "# Importing Relevant Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d3a7f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as Datasets\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce948f56",
   "metadata": {},
   "source": [
    "# Defining dataset paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "816b8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './imgs/training'\n",
    "validation_dataset_path = './imgs/validation'\n",
    "test_dataset_path = './imgs/test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac914b12",
   "metadata": {},
   "source": [
    "# Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c3a88dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f845edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal note on transforms.Compose:\n",
    "\n",
    "#   This composes all of the transforms to be applied together\n",
    "#   First we resi`e all the images to a guven size. This is because CNN requires all input images to be of the\n",
    "#   same size to\n",
    "#   work properly\n",
    "#   We alse tranform all the images to tensors. In pytorchs .ToTensors(), it changes the images to (N, C, H, W),\n",
    "#   where  is the\n",
    "#   the batch size, C is channel, H is height and W is width\n",
    "\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4c16af5",
   "metadata": {},
   "source": [
    "# Defining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc008adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasets.ImageFolder(root = train_dataset_path, transform = transform)\n",
    "validation_dataset = Datasets.ImageFolder(root = validation_dataset_path, transform = transform) \n",
    "test_dataset = Datasets.ImageFolder(root = test_dataset_path, transform = transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0397b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Note on Datsets.ImageFolder:\n",
    "\n",
    "#   root (string) – Root directory path.\n",
    "#   transform (callable, optional) – A function/transform that takes in an image and returns a transformed\n",
    "#   version\n",
    "\n",
    "#   We can use these datasets to create our iterable data loaders.\n",
    "\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccea5b69",
   "metadata": {},
   "source": [
    "# Making Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c120e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 12, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size = 12, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14e3668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Note on orch.utils.data.DataLoader:\n",
    "\n",
    "#   Below, under the Training functions heading you can see we use map-style dataloader\n",
    "#   such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label\n",
    "#   from a folder \n",
    "#   on the disk.\n",
    "#   DataLoader supports automatically collating individual fetched data samples into batches via arguments\n",
    "#   batch_size\n",
    "#   \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82665a68",
   "metadata": {},
   "source": [
    "# Configuring Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "482d7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # this ensures that the current MacOS version is at least 12.3+\n",
    "# print(torch.backends.mps.is_available())\n",
    "# # this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "# print(torch.backends.mps.is_built())\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('Running on the CPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2528aaf0",
   "metadata": {},
   "source": [
    "# Defining Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "236d29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class encoder(nn.Module):\n",
    "  def __init__(self, out_channels=16, latent_dim=256):\n",
    "    super().__init__()\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 16, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, padding=1,stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, stride = 2, kernel_size = 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, 3, padding=1, stride =2), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.Conv2d(128, 128, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*out_channels*8*8, latent_dim),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 3, 128, 128)\n",
    "    output = self.net(x)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "54dc4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# Personal Note on encoder:\n",
    "\n",
    "#The dropout layer has been added to prevent over fitting. There are two dropout layers at the end of the\n",
    "#encoder and the two add the start of the decoder all at a dropout rate of 0.05\n",
    "\n",
    "#The dropoiut layer randomly disactivates some neurons, in this case random 5% of neurons.\n",
    "\n",
    "#the 32x32 image is porcessed until it is an 8x8 feature map and then is flattened to a vector of 4096.\n",
    "# A linear layer is applied to comporess it to a previously defined latent dimension of 228. This latent\n",
    "# dimension is the size of the bottleneck\n",
    "\n",
    "# The linear layer is also know as the fully connected layer\n",
    "# this layer helps in changing dimesionality of the output from the preceding layer. The input features\n",
    "# are received by a linear layer are passed in the form of a flattned oe dimension tensor and the multiplied\n",
    "# by a weight matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eddc7bf2",
   "metadata": {},
   "source": [
    "# Defining Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "857a6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=256):\n",
    "    super().__init__()\n",
    "\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "    self.linear = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 32*out_channels*8*8),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(8*out_channels, 8*out_channels, 3, padding=1), # (8, 8)\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.ConvTranspose2d(8*out_channels, 4*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.ConvTranspose2d(4*out_channels, 4*out_channels, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(2*out_channels, out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.linear(x)\n",
    "    output = output.view(-1, 128, 16, 16)\n",
    "    output = self.conv(output)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c66e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# Personal Note on decoder:\n",
    "\n",
    "#The first thing that needs to be done is to add the linear layer. So under the \"def forward(self, x)\",\n",
    "# we apply the linear layer and then the conv layers\n",
    "\n",
    "#.view(): \n",
    "#   It reshapes the tenosr without copying memory\n",
    "#   The \"-1\", is when there is any situation where you dont know how many rows you want but are sure of\n",
    "#   the number of \n",
    "#   columns. Asking \" Give me a tensor that has these many columns and you compute the appropriate number\n",
    "#   of rows to make this happen \"\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988dfa04",
   "metadata": {},
   "source": [
    "# Defining Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ee2348ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  defining autoencoder\n",
    "class autoencoder(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.encoder.to(device)\n",
    "    \n",
    "    self.decoder = decoder\n",
    "    self.decoder.to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a947a72",
   "metadata": {},
   "source": [
    "# Summarizing Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4da8b009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 128, 128]             448\n",
      "              ReLU-2         [-1, 16, 128, 128]               0\n",
      "            Conv2d-3         [-1, 16, 128, 128]           2,320\n",
      "              ReLU-4         [-1, 16, 128, 128]               0\n",
      "            Conv2d-5           [-1, 32, 64, 64]           4,640\n",
      "              ReLU-6           [-1, 32, 64, 64]               0\n",
      "            Conv2d-7           [-1, 32, 64, 64]           9,248\n",
      "              ReLU-8           [-1, 32, 64, 64]               0\n",
      "            Conv2d-9           [-1, 64, 32, 32]          18,496\n",
      "             ReLU-10           [-1, 64, 32, 32]               0\n",
      "           Conv2d-11           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 16, 16]          73,856\n",
      "             ReLU-14          [-1, 128, 16, 16]               0\n",
      "          Dropout-15          [-1, 128, 16, 16]               0\n",
      "           Conv2d-16          [-1, 128, 16, 16]         147,584\n",
      "             ReLU-17          [-1, 128, 16, 16]               0\n",
      "          Dropout-18          [-1, 128, 16, 16]               0\n",
      "          Flatten-19                [-1, 32768]               0\n",
      "           Linear-20                  [-1, 256]       8,388,864\n",
      "             ReLU-21                  [-1, 256]               0\n",
      "================================================================\n",
      "Total params: 8,682,384\n",
      "Trainable params: 8,682,384\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 15.75\n",
      "Params size (MB): 33.12\n",
      "Estimated Total Size (MB): 49.06\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 1, 32768]       8,421,376\n",
      "              ReLU-2             [-1, 1, 32768]               0\n",
      "   ConvTranspose2d-3          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-4          [-1, 128, 16, 16]               0\n",
      "           Dropout-5          [-1, 128, 16, 16]               0\n",
      "   ConvTranspose2d-6           [-1, 64, 32, 32]          73,792\n",
      "              ReLU-7           [-1, 64, 32, 32]               0\n",
      "           Dropout-8           [-1, 64, 32, 32]               0\n",
      "   ConvTranspose2d-9           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-10           [-1, 64, 32, 32]               0\n",
      "  ConvTranspose2d-11           [-1, 32, 64, 64]          18,464\n",
      "             ReLU-12           [-1, 32, 64, 64]               0\n",
      "  ConvTranspose2d-13           [-1, 32, 64, 64]           9,248\n",
      "             ReLU-14           [-1, 32, 64, 64]               0\n",
      "  ConvTranspose2d-15         [-1, 16, 128, 128]           4,624\n",
      "             ReLU-16         [-1, 16, 128, 128]               0\n",
      "  ConvTranspose2d-17         [-1, 16, 128, 128]           2,320\n",
      "             ReLU-18         [-1, 16, 128, 128]               0\n",
      "  ConvTranspose2d-19          [-1, 3, 128, 128]             435\n",
      "================================================================\n",
      "Total params: 8,714,771\n",
      "Trainable params: 8,714,771\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 16.12\n",
      "Params size (MB): 33.24\n",
      "Estimated Total Size (MB): 49.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "enc = encoder().to(device)\n",
    "summary(enc, input_size=(1, 3, 128, 128))\n",
    "dec = decoder().to(device)\n",
    "summary(dec, input_size=(1, 256))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a97adba5",
   "metadata": {},
   "source": [
    "# Training model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d6c90a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model \n",
    "# encdr=encoder()\n",
    "model = autoencoder(encoder(), decoder())\n",
    "\n",
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 3, 128, 128)\n",
    "    save_image(img, name)\n",
    "    \n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "criterion = nn.MSELoss() #Mean error loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) \n",
    "\n",
    "def train(net, trainloader, validationloader, NUM_EPOCHS):\n",
    "\n",
    "    train_loss = [] #record the train loss after each epoch\n",
    "    val_loss = [] #record the validation loss after each epoch\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        running_loss_val = 0.0\n",
    "\n",
    "        ################ TRAINING ################\n",
    "        #iterating through the training loader\n",
    "        for data in trainloader: \n",
    "            img, _ = data # no need for the labels\n",
    "            img = img.to(device)\n",
    "            optimizer.zero_grad() # setting gradients of all the optimize tensors to zer\n",
    "            outputs = net(img) # putting image through the cnn\n",
    "            loss = criterion(outputs, img) #calculating loss using mean squared error\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        ############### VALIDATION ###############\n",
    "\n",
    "        for val_data in validationloader:\n",
    "            img_val, _ = val_data\n",
    "            img_val = img_val.to(device)\n",
    "            output_val = net(img_val)\n",
    "            loss_val = criterion(output_val, img_val)\n",
    "            running_loss_val += loss_val.item()\n",
    "\n",
    "        loss = running_loss / len(trainloader)\n",
    "        loss_val = running_loss_val/ len(validation_loader)\n",
    "\n",
    "        val_loss.append(loss_val)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}, Validation Loss: {:.3f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, loss, loss_val))\n",
    "        if epoch % 5 == 0:\n",
    "            save_decoded_image(img.cpu().data, name='./Images_train/original{}.png'.format(epoch))\n",
    "            save_decoded_image(outputs.cpu().data, name='./Images_train/decoded{}.png'.format(epoch))\n",
    "            save_decoded_image(img_val.cpu().data, name='./Images_validation/original_validation{}.png'.format(epoch))\n",
    "            save_decoded_image(output_val.cpu().data, name='./Images_validation/decoded_validation{}.png'.format(epoch))\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def test_image_reconstruction(net, testloader, encoder):\n",
    "     for batch in testloader:\n",
    "        img, _ = batch\n",
    "        img = img.to(device)\n",
    "        img_code = encoder(img)\n",
    "        # print('CODE:')\n",
    "        # print(img_code)\n",
    "        # print('EXIT CODE')\n",
    "        outputs = net(img)\n",
    "        outputs = outputs.view(outputs.size(0), 3, 128, 128).cpu().data\n",
    "        save_image(img, 'original_images.png')\n",
    "        save_image(outputs, 'reconstructed_images.png')\n",
    "        break\n",
    "     return img_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3ff2ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Notes:\n",
    "\n",
    "#Adam optimizer: The adam optimizer is sed under the gradient decent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8c38ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lamiayous/Projects/FindMe/myenv/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20, Train Loss: 0.071, Validation Loss: 0.035\n",
      "Epoch 2 of 20, Train Loss: 0.035, Validation Loss: 0.029\n",
      "Epoch 3 of 20, Train Loss: 0.030, Validation Loss: 0.027\n",
      "Epoch 4 of 20, Train Loss: 0.028, Validation Loss: 0.024\n",
      "Epoch 5 of 20, Train Loss: 0.026, Validation Loss: 0.024\n",
      "Epoch 6 of 20, Train Loss: 0.024, Validation Loss: 0.023\n",
      "Epoch 7 of 20, Train Loss: 0.022, Validation Loss: 0.020\n",
      "Epoch 8 of 20, Train Loss: 0.020, Validation Loss: 0.019\n",
      "Epoch 9 of 20, Train Loss: 0.019, Validation Loss: 0.017\n",
      "Epoch 10 of 20, Train Loss: 0.018, Validation Loss: 0.018\n",
      "Epoch 11 of 20, Train Loss: 0.017, Validation Loss: 0.017\n",
      "Epoch 12 of 20, Train Loss: 0.017, Validation Loss: 0.016\n",
      "Epoch 13 of 20, Train Loss: 0.017, Validation Loss: 0.016\n",
      "Epoch 14 of 20, Train Loss: 0.016, Validation Loss: 0.016\n",
      "Epoch 15 of 20, Train Loss: 0.016, Validation Loss: 0.017\n",
      "Epoch 16 of 20, Train Loss: 0.016, Validation Loss: 0.015\n",
      "Epoch 17 of 20, Train Loss: 0.016, Validation Loss: 0.016\n",
      "Epoch 18 of 20, Train Loss: 0.015, Validation Loss: 0.017\n",
      "Epoch 19 of 20, Train Loss: 0.015, Validation Loss: 0.015\n",
      "Epoch 20 of 20, Train Loss: 0.015, Validation Loss: 0.015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Making directory to store the Images\n",
    "def make_dir():\n",
    "    train_image_dir = 'Images_train'\n",
    "    val_image_dir = 'Images_validation'\n",
    "    if not os.path.exists(train_image_dir):\n",
    "        os.makedirs(train_image_dir)\n",
    "    if not os.path.exists(val_image_dir):\n",
    "        os.makedirs(val_image_dir)\n",
    "model.train()\n",
    "model.to(device)\n",
    "make_dir()\n",
    "train_loss, val_loss = train(model, training_loader, validation_loader, 20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "172bdc34",
   "metadata": {},
   "source": [
    "# Unique code function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c38584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6139a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_code_files(img_codes):\n",
    "    codes_arr = torch.detach(img_codes).numpy()\n",
    "    no_imgs = len(codes_arr)\n",
    "    index = 0 \n",
    "\n",
    "    while index < no_imgs:\n",
    "        img_no = str(index)\n",
    "        filename = (\"code_img_\" + img_no + \".txt\")\n",
    "        np.savetxt(filename, codes_arr[index])\n",
    "        index += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618a3961",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m autoencoder(encoder(), decoder())\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39meval() \u001b[39m#turning off all dropout layer for inference\u001b[39;00m\n\u001b[1;32m      4\u001b[0m img_codes \u001b[39m=\u001b[39m test_image_reconstruction(model, test_loader, encoder())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval() #turning off all dropout layer for inference\n",
    "\n",
    "img_codes = test_image_reconstruction(model, test_loader, encoder())\n",
    "unique_code_files(img_codes)\n",
    "# img = torch.detach(img_code).numpy()\n",
    "# np.savetxt('CODE.txt', torch.detach(img_code).numpy())\n",
    "#plotting training and validation results \n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss)\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('training_loss.png')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('validation_loss.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
