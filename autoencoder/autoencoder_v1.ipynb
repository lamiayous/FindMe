{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f44ea31b",
   "metadata": {},
   "source": [
    "# Importing Relevant Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d3a7f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as Datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce948f56",
   "metadata": {},
   "source": [
    "# Defining dataset paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "816b8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './imgs/training'\n",
    "validation_dataset_path = './imgs/validation'\n",
    "test_dataset_path = './imgs/test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac914b12",
   "metadata": {},
   "source": [
    "# Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c3a88dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f845edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal note on transforms.Compose:\n",
    "\n",
    "#   This composes all of the transforms to be applied together\n",
    "#   First we resi`e all the images to a guven size. This is because CNN requires all input images to be of the\n",
    "#   same size to\n",
    "#   work properly\n",
    "#   We alse tranform all the images to tensors. In pytorchs .ToTensors(), it changes the images to (N, C, H, W),\n",
    "#   where  is the\n",
    "#   the batch size, C is channel, H is height and W is width\n",
    "\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4c16af5",
   "metadata": {},
   "source": [
    "# Defining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bc008adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasets.ImageFolder(root = train_dataset_path, transform = transform)\n",
    "validation_dataset = Datasets.ImageFolder(root = validation_dataset_path, transform = transform) \n",
    "test_dataset = Datasets.ImageFolder(root = test_dataset_path, transform = transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0397b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Note on Datsets.ImageFolder:\n",
    "\n",
    "#   root (string) – Root directory path.\n",
    "#   transform (callable, optional) – A function/transform that takes in an image and returns a transformed\n",
    "#   version\n",
    "\n",
    "#   We can use these datasets to create our iterable data loaders.\n",
    "\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccea5b69",
   "metadata": {},
   "source": [
    "# Making Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3c120e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 12, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size = 12, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "14e3668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Note on orch.utils.data.DataLoader:\n",
    "\n",
    "#   Below, under the Training functions heading you can see we use map-style dataloader\n",
    "#   such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label\n",
    "#   from a folder \n",
    "#   on the disk.\n",
    "#   DataLoader supports automatically collating individual fetched data samples into batches via arguments\n",
    "#   batch_size\n",
    "#   \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82665a68",
   "metadata": {},
   "source": [
    "# Configuring Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "482d7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # this ensures that the current MacOS version is at least 12.3+\n",
    "# print(torch.backends.mps.is_available())\n",
    "# # this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "# print(torch.backends.mps.is_built())\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('Running on the CPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2528aaf0",
   "metadata": {},
   "source": [
    "# Defining Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "236d29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class encoder(nn.Module):\n",
    "  def __init__(self, out_channels=16, latent_dim=256):\n",
    "    super().__init__()\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 16, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, padding=1,stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, stride = 2, kernel_size = 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, 3, padding=1, stride =2), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.Conv2d(128, 128, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*out_channels*8*8, latent_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 3, 128, 128)\n",
    "    output = self.net(x)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "54dc4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# Personal Note on encoder:\n",
    "\n",
    "#The dropout layer has been added to prevent over fitting. There are two dropout layers at the end of the\n",
    "#encoder and the two add the start of the decoder all at a dropout rate of 0.05\n",
    "\n",
    "#The dropoiut layer randomly disactivates some neurons, in this case random 5% of neurons.\n",
    "\n",
    "#the 32x32 image is porcessed until it is an 8x8 feature map and then is flattened to a vector of 4096.\n",
    "# A linear layer is applied to comporess it to a previously defined latent dimension of 228. This latent\n",
    "# dimension is the size of the bottleneck\n",
    "\n",
    "# The linear layer is also know as the fully connected layer\n",
    "# this layer helps in changing dimesionality of the output from the preceding layer. The input features\n",
    "# are received by a linear layer are passed in the form of a flattned oe dimension tensor and the multiplied\n",
    "# by a weight matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eddc7bf2",
   "metadata": {},
   "source": [
    "# Defining Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "857a6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=256):\n",
    "    super().__init__()\n",
    "\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "    self.linear = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 32*out_channels*8*8),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(8*out_channels, 8*out_channels, 3, padding=1), # (8, 8)\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.ConvTranspose2d(8*out_channels, 4*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.ConvTranspose2d(4*out_channels, 4*out_channels, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(2*out_channels, out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.linear(x)\n",
    "    output = output.view(-1, 128, 16, 16)\n",
    "    output = self.conv(output)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2c66e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# Personal Note on decoder:\n",
    "\n",
    "#The first thing that needs to be done is to add the linear layer. So under the \"def forward(self, x)\",\n",
    "# we apply the linear layer and then the conv layers\n",
    "\n",
    "#.view(): \n",
    "#   It reshapes the tenosr without copying memory\n",
    "#   The \"-1\", is when there is any situation where you dont know how many rows you want but are sure of\n",
    "#   the number of \n",
    "#   columns. Asking \" Give me a tensor that has these many columns and you compute the appropriate number\n",
    "#   of rows to make this happen \"\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988dfa04",
   "metadata": {},
   "source": [
    "# Defining Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ee2348ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  defining autoencoder\n",
    "class autoencoder(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.encoder.to(device)\n",
    "    \n",
    "    self.decoder = decoder\n",
    "    self.decoder.to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a947a72",
   "metadata": {},
   "source": [
    "# Summarizing Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4da8b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder().to(device)\n",
    "# summary(enc, input_size=(1, 3, 128, 128))\n",
    "dec = decoder().to(device)\n",
    "# summary(dec, input_size=(1, 256))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a97adba5",
   "metadata": {},
   "source": [
    "# Training model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d6c90a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model \n",
    "model = autoencoder(encoder(), decoder())\n",
    "\n",
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 3, 128, 128)\n",
    "    save_image(img, name)\n",
    "    \n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "criterion = nn.MSELoss() #Mean error loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) \n",
    "\n",
    "def train(net, trainloader, validationloader, NUM_EPOCHS):\n",
    "\n",
    "    train_loss = [] #record the train loss after each epoch\n",
    "    val_loss = [] #record the validation loss after each epoch\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        running_loss_val = 0.0\n",
    "\n",
    "        ################ TRAINING ################\n",
    "        #iterating through the training loader\n",
    "        for data in trainloader: \n",
    "            img, _ = data # no need for the labels\n",
    "            img = img.to(device)\n",
    "            optimizer.zero_grad() # setting gradients of all the optimize tensors to zer\n",
    "            outputs = net(img) # putting image through the cnn\n",
    "            loss = criterion(outputs, img) #calculating loss using mean squared error\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        ############### VALIDATION ###############\n",
    "\n",
    "        for val_data in validationloader:\n",
    "            img_val, _ = val_data\n",
    "            img_val = img_val.to(device)\n",
    "            output_val = net(img_val)\n",
    "            loss_val = criterion(output_val, img_val)\n",
    "            running_loss_val += loss_val.item()\n",
    "\n",
    "        loss = running_loss / len(trainloader)\n",
    "        loss_val = running_loss_val/ len(validation_loader)\n",
    "\n",
    "        val_loss.append(loss_val)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}, Validation Loss: {:.3f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, loss, loss_val))\n",
    "        if epoch % 5 == 0:\n",
    "            save_decoded_image(img.cpu().data, name='./Images_train/original{}.png'.format(epoch))\n",
    "            save_decoded_image(outputs.cpu().data, name='./Images_train/decoded{}.png'.format(epoch))\n",
    "            save_decoded_image(img_val.cpu().data, name='./Images_validation/original_validation{}.png'.format(epoch))\n",
    "            save_decoded_image(output_val.cpu().data, name='./Images_validation/decoded_validation{}.png'.format(epoch))\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def test_image_reconstruction(net, testloader, encoder):\n",
    "     for batch in testloader:\n",
    "        img, _ = batch\n",
    "        img = img.to(device)\n",
    "        img_code = encoder(img)\n",
    "        outputs = net(img)\n",
    "        outputs = outputs.view(outputs.size(0), 3, 128, 128).cpu().data\n",
    "        save_image(img, 'original_images.png')\n",
    "        save_image(outputs, 'reconstructed_images.png')\n",
    "        break\n",
    "     return img_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3ff2ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Notes:\n",
    "\n",
    "#Adam optimizer: The adam optimizer is sed under the gradient decent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8c38ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lamiayous/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20, Train Loss: 0.096, Validation Loss: 0.091\n",
      "Epoch 2 of 20, Train Loss: 0.087, Validation Loss: 0.084\n",
      "Epoch 3 of 20, Train Loss: 0.086, Validation Loss: 0.085\n",
      "Epoch 4 of 20, Train Loss: 0.086, Validation Loss: 0.084\n",
      "Epoch 5 of 20, Train Loss: 0.086, Validation Loss: 0.084\n",
      "Epoch 6 of 20, Train Loss: 0.086, Validation Loss: 0.084\n",
      "Epoch 7 of 20, Train Loss: 0.086, Validation Loss: 0.084\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[233], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m make_dir()\n\u001b[0;32m---> 12\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m train(model, training_loader, validation_loader, \u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[231], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, trainloader, validationloader, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     19\u001b[0m running_loss_val \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39m################ TRAINING ################\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#iterating through the training loader\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m trainloader: \n\u001b[1;32m     24\u001b[0m     img, _ \u001b[39m=\u001b[39m data \u001b[39m# no need for the labels\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    247\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/PIL/Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[1;32m    864\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[1;32m    865\u001b[0m ):\n\u001b[1;32m    866\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    913\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    915\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#Making directory to store the Images\n",
    "def make_dir():\n",
    "    train_image_dir = 'Images_train'\n",
    "    val_image_dir = 'Images_validation'\n",
    "    if not os.path.exists(train_image_dir):\n",
    "        os.makedirs(train_image_dir)\n",
    "    if not os.path.exists(val_image_dir):\n",
    "        os.makedirs(val_image_dir)\n",
    "model.train()\n",
    "model.to(device)\n",
    "make_dir()\n",
    "train_loss, val_loss = train(model, training_loader, validation_loader, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"last.pth\"\n",
    "torch.save(model.state_dict(), FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
