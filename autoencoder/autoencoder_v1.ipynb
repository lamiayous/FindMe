{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f44ea31b",
   "metadata": {},
   "source": [
    "# Importing Relevant Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3a7f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as Datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce948f56",
   "metadata": {},
   "source": [
    "# Defining dataset paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816b8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './imgs/training'\n",
    "validation_dataset_path = './imgs/validation'\n",
    "test_dataset_path = './imgs/test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac914b12",
   "metadata": {},
   "source": [
    "# Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a88dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()], transforms.Normlaize((0.5, 0.5,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f845edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal note on transforms.Compose:\n",
    "\n",
    "#   This composes all of the transforms to be applied together\n",
    "#   First we resi`e all the images to a guven size. This is because CNN requires all input images to be of the\n",
    "#   same size to\n",
    "#   work properly\n",
    "#   We alse tranform all the images to tensors. In pytorchs .ToTensors(), it changes the images to (N, C, H, W),\n",
    "#   where  is the\n",
    "#   the batch size, C is channel, H is height and W is width\n",
    "\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4c16af5",
   "metadata": {},
   "source": [
    "# Defining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc008adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasets.ImageFolder(root = train_dataset_path, transform = transform)\n",
    "validation_dataset = Datasets.ImageFolder(root = validation_dataset_path, transform = transform) \n",
    "test_dataset = Datasets.ImageFolder(root = test_dataset_path, transform = transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0397b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Note on Datsets.ImageFolder:\n",
    "\n",
    "#   root (string) – Root directory path.\n",
    "#   transform (callable, optional) – A function/transform that takes in an image and returns a transformed\n",
    "#   version\n",
    "\n",
    "#   We can use these datasets to create our iterable data loaders.\n",
    "\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccea5b69",
   "metadata": {},
   "source": [
    "# Making Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c120e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 12, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size = 12, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "14e3668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Note on orch.utils.data.DataLoader:\n",
    "\n",
    "#   Below, under the Training functions heading you can see we use map-style dataloader\n",
    "#   such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label\n",
    "#   from a folder \n",
    "#   on the disk.\n",
    "#   DataLoader supports automatically collating individual fetched data samples into batches via arguments\n",
    "#   batch_size\n",
    "#   \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82665a68",
   "metadata": {},
   "source": [
    "# Configuring Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "482d7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # this ensures that the current MacOS version is at least 12.3+\n",
    "# print(torch.backends.mps.is_available())\n",
    "# # this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "# print(torch.backends.mps.is_built())\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('Running on the CPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2528aaf0",
   "metadata": {},
   "source": [
    "# Defining Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "236d29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class encoder(nn.Module):\n",
    "  def __init__(self, out_channels=16, latent_dim=256):\n",
    "    super().__init__()\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 16, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, padding=1,stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, stride = 2, kernel_size = 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, 3, padding=1, stride =2), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.Conv2d(128, 128, 3, padding=1), \n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*out_channels*8*8, latent_dim),\n",
    "\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 3, 128, 128)\n",
    "    output = self.net(x)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54dc4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# Personal Note on encoder:\n",
    "\n",
    "#The dropout layer has been added to prevent over fitting. There are two dropout layers at the end of the\n",
    "#encoder and the two add the start of the decoder all at a dropout rate of 0.05\n",
    "\n",
    "#The dropoiut layer randomly disactivates some neurons, in this case random 5% of neurons.\n",
    "\n",
    "#the 32x32 image is porcessed until it is an 8x8 feature map and then is flattened to a vector of 4096.\n",
    "# A linear layer is applied to comporess it to a previously defined latent dimension of 228. This latent\n",
    "# dimension is the size of the bottleneck\n",
    "\n",
    "# The linear layer is also know as the fully connected layer\n",
    "# this layer helps in changing dimesionality of the output from the preceding layer. The input features\n",
    "# are received by a linear layer are passed in the form of a flattned oe dimension tensor and the multiplied\n",
    "# by a weight matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eddc7bf2",
   "metadata": {},
   "source": [
    "# Defining Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "857a6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=256):\n",
    "    super().__init__()\n",
    "\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "    self.linear = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 32*out_channels*8*8),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(8*out_channels, 8*out_channels, 3, padding=1), # (8, 8)\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.ConvTranspose2d(8*out_channels, 4*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.05),\n",
    "        nn.ConvTranspose2d(4*out_channels, 4*out_channels, 3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(2*out_channels, out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.linear(x)\n",
    "    output = output.view(-1, 128, 16, 16)\n",
    "    output = self.conv(output)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c66e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# Personal Note on decoder:\n",
    "\n",
    "#The first thing that needs to be done is to add the linear layer. So under the \"def forward(self, x)\",\n",
    "# we apply the linear layer and then the conv layers\n",
    "\n",
    "#.view(): \n",
    "#   It reshapes the tenosr without copying memory\n",
    "#   The \"-1\", is when there is any situation where you dont know how many rows you want but are sure of\n",
    "#   the number of \n",
    "#   columns. Asking \" Give me a tensor that has these many columns and you compute the appropriate number\n",
    "#   of rows to make this happen \"\n",
    "#########################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988dfa04",
   "metadata": {},
   "source": [
    "# Defining Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee2348ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  defining autoencoder\n",
    "class autoencoder(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.encoder.to(device)\n",
    "    \n",
    "    self.decoder = decoder\n",
    "    self.decoder.to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a947a72",
   "metadata": {},
   "source": [
    "# Summarizing Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da8b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder().to(device)\n",
    "summary(enc, input_size=(1, 3, 128, 128))\n",
    "dec = decoder().to(device)\n",
    "summary(dec, input_size=(1, 256))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a97adba5",
   "metadata": {},
   "source": [
    "# Training model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6c90a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model \n",
    "model = autoencoder(encoder(), decoder())\n",
    "\n",
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 3, 128, 128)\n",
    "    save_image(img, name)\n",
    "    \n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "criterion = nn.MSELoss() #Mean error loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) \n",
    "\n",
    "def train(net, trainloader, validationloader, NUM_EPOCHS):\n",
    "\n",
    "    train_loss = [] #record the train loss after each epoch\n",
    "    val_loss = [] #record the validation loss after each epoch\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        running_loss_val = 0.0\n",
    "\n",
    "        ################ TRAINING ################\n",
    "        #iterating through the training loader\n",
    "        for data in trainloader: \n",
    "            img, _ = data # no need for the labels\n",
    "            img = img.to(device)\n",
    "            optimizer.zero_grad() # setting gradients of all the optimize tensors to zer\n",
    "            outputs = net(img) # putting image through the cnn\n",
    "            loss = criterion(outputs, img) #calculating loss using mean squared error\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        ############### VALIDATION ###############\n",
    "\n",
    "        for val_data in validationloader:\n",
    "            img_val, _ = val_data\n",
    "            img_val = img_val.to(device)\n",
    "            output_val = net(img_val)\n",
    "            loss_val = criterion(output_val, img_val)\n",
    "            running_loss_val += loss_val.item()\n",
    "\n",
    "        loss = running_loss / len(trainloader)\n",
    "        loss_val = running_loss_val/ len(validation_loader)\n",
    "\n",
    "        val_loss.append(loss_val)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}, Validation Loss: {:.3f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, loss, loss_val))\n",
    "        if epoch % 5 == 0:\n",
    "            save_decoded_image(img.cpu().data, name='./Images_train/original{}.png'.format(epoch))\n",
    "            save_decoded_image(outputs.cpu().data, name='./Images_train/decoded{}.png'.format(epoch))\n",
    "            save_decoded_image(img_val.cpu().data, name='./Images_validation/original_validation{}.png'.format(epoch))\n",
    "            save_decoded_image(output_val.cpu().data, name='./Images_validation/decoded_validation{}.png'.format(epoch))\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def test_image_reconstruction(net, testloader, encoder):\n",
    "     for batch in testloader:\n",
    "        img, _ = batch\n",
    "        img = img.to(device)\n",
    "        img_code = encoder(img)\n",
    "        outputs = net(img)\n",
    "        outputs = outputs.view(outputs.size(0), 3, 128, 128).cpu().data\n",
    "        save_image(img, 'original_images.png')\n",
    "        save_image(outputs, 'reconstructed_images.png')\n",
    "        break\n",
    "     return img_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3ff2ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Personal Notes:\n",
    "\n",
    "#Adam optimizer: The adam optimizer is sed under the gradient decent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c38ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m make_dir()\n\u001b[0;32m---> 12\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m train(model, training_loader, validation_loader, \u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, trainloader, validationloader, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     25\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# setting gradients of all the optimize tensors to zer\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[39m=\u001b[39m net(img) \u001b[39m# putting image through the cnn\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, img) \u001b[39m#calculating loss using mean squared error\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mautoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m   encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[0;32m---> 13\u001b[0m   decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(encoded)\n\u001b[1;32m     14\u001b[0m   \u001b[39mreturn\u001b[39;00m decoded\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mdecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x)\n\u001b[1;32m     37\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m16\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(output)\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/Projects/FindMe/object_detector/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#Making directory to store the Images\n",
    "def make_dir():\n",
    "    train_image_dir = 'Images_train'\n",
    "    val_image_dir = 'Images_validation'\n",
    "    if not os.path.exists(train_image_dir):\n",
    "        os.makedirs(train_image_dir)\n",
    "    if not os.path.exists(val_image_dir):\n",
    "        os.makedirs(val_image_dir)\n",
    "model.train()\n",
    "model.to(device)\n",
    "make_dir()\n",
    "train_loss, val_loss = train(model, training_loader, validation_loader, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b07063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"last.pth\"\n",
    "torch.save(model.state_dict(), FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
